quarkus.http.port = 8080

# PostgreSQL database automatically created by Dev Services with the following settings
quarkus.devservices.enabled = true
quarkus.datasource.devservices.port = 5432
quarkus.datasource.devservices.db-name = rag
quarkus.datasource.devservices.username = rag
quarkus.datasource.devservices.password = rag
# Run Flyway database schema migrations automatically
quarkus.flyway.migrate-at-start = true

# The dimension of the embedding vectors. This has to be the same as the dimension of vectors produced by the embedding model that you use.
quarkus.langchain4j.pgvector.dimension = 768
# The table name for storing embeddings - see V1__create_schema.sql
quarkus.langchain4j.pgvector.table = embedding_item
# Used if the table and index are created automatically by langchain4j
quarkus.langchain4j.pgvector.use-index = true
quarkus.langchain4j.pgvector.index-list-size= 100

quarkus.langchain4j.log-requests = true
quarkus.langchain4j.log-responses = true
# The temperature to use for the chat model. Temperature is a value between 0 and 1, where lower values make the model more deterministic and higher values make it more creative.
quarkus.langchain4j.temperature = 0.2
# Global timeout for requests to LLM APIs
quarkus.langchain4j.timeout = 120s

# The chat model to use. In case of Ollama, llama3.1 is the default chat model.
quarkus.langchain4j.ollama.chat-model.model-id = gpt-oss
# The format to return a response in. Format can be json or a JSON schema, or text; in this application, we use text.
quarkus.langchain4j.ollama.chat-model.format = text
# In case of Ollama, nomic-embed-text is the default model used for text embeddings.
quarkus.langchain4j.ollama.embedding-model.model-id = nomic-embed-text
# Whether embedding model requests should be logged; default is false
quarkus.langchain4j.ollama.embedding-model.log-requests = true
# Whether embedding model responses should be logged; default is false
quarkus.langchain4j.ollama.embedding-model.log-responses=true

# The location of the documents to be processed; can be relative or absolute path.
rag.document.location = ./documents
# The document loader scheduler period; default is 60 seconds
rag.document.loader.scheduler.period = 10s
# The maximum length (in characters) of a document segment used during ingestion. Default is 550 characters.
rag.embedding.max-segment-size = 500
# The maximum number of characters that can overlap between two segments. Default is 25 characters.
rag.embedding.max-overlap-size = 25
# The maximum number of retrieved embeddings when querying for relevant documents. Default is 200.
rag.retrieval.max-results = 200
# The minimum cosine similarity score for a document to be considered relevant during retrieval. Score ranges between 0 (no similarity) and 1 (identical). Default is 0.8.
rag.retrieval.min-score = 0.8

# LangFuse OpenTelemetry settings
quarkus.otel.enabled = true
quarkus.otel.metrics.enabled = true
# OpenTelemetry defines the encoding of telemetry data and the protocol used to exchange data between the client and the server. Default is grpc.
quarkus.otel.exporter.otlp.traces.protocol = http/protobuf
# LangFuse OpenTelemetry endpoint and authorization header
quarkus.otel.exporter.otlp.traces.headers = Authorization=Basic ***
quarkus.otel.exporter.otlp.traces.endpoint = http://localhost:3000/api/public/otel
quarkus.langchain4j.tracing.include-prompt = true
quarkus.langchain4j.tracing.include-completion = true

# Grafana and OpenTelemetry ports for LGTM observability; by default testcontainers exposes these on random ports.
quarkus.observability.lgtm.grafana-port = 3001
quarkus.observability.lgtm.otel-grpc-port = 5317
quarkus.observability.lgtm.otel-http-port = 5318
# Grafana LGTM metrics
quarkus.otel.exporter.otlp.metrics.endpoint = http://localhost:5318
quarkus.otel.exporter.otlp.metrics.protocol = http/protobuf
#Grafana LGTM logs
quarkus.otel.exporter.otlp.logs.endpoint = http://localhost:5318
quarkus.otel.exporter.otlp.logs.protocol = http/protobuf
